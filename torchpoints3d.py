# -*- coding: utf-8 -*-
"""Torchpoints3d.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1omEdmmc06EPYzXOWp4-3LGcediwlyER3
"""

# Setup packages can take some time (30 minutes or so)
# !pip install pyvista pytorch-lightning
# !pip install --upgrade jsonschema
# !pip install torch-points3d
# !apt-get install -qq xvfb libgl1-mesa-glx

import os
import sys
from omegaconf import OmegaConf
# import pyvista as pv
import torch
import time
import datetime

import torch_points3d

DIR = "/content/drive/MyDrive/IFT3150/Object_Classification/"

MODELNET_VERSION="40" #@param ["10", "40"]
USE_NORMAL = True #@param {type:"boolean"}

from torch_points3d.applications.rsconv import RSConv
from torch_points3d.applications.pointnet2 import PointNet2
from torch_points3d.applications.kpconv import KPConv

class RSConvCLassifier(torch.nn.Module):
    def __init__(self):
        super().__init__() 
        self.encoder = RSConv("encoder", input_nc= 3 * USE_NORMAL,output_nc = int(MODELNET_VERSION), num_layers=4)
        self.log_softmax = torch.nn.LogSoftmax(dim=-1)
    
    @property
    def conv_type(self):
        """ This is needed by the dataset to infer which batch collate should be used"""
        return self.encoder.conv_type
    
    def get_output(self):
        """ This is needed by the tracker to get access to the ouputs of the network"""
        return self.output
    
    def get_labels(self):
        """ Needed by the tracker in order to access ground truth labels"""
        return self.labels
    
    def get_current_losses(self):
        """ Entry point for the tracker to grab the loss """
        return {"loss_class": float(self.loss_class)}
    
    def forward(self, data):
        # Set labels for the tracker
        self.labels = data.y.squeeze()

        # Forward through the network
        data_out = self.encoder(data)
        self.output = self.log_softmax(data_out.x.squeeze())

        # Set loss for the backward pass
        self.loss_class = torch.nn.functional.nll_loss(self.output, self.labels)
    
    def backward(self):
         self.loss_class.backward()


class PointNet2CLassifier(torch.nn.Module):
    def __init__(self):
        super().__init__() 
        self.encoder = PointNet2(
                        architecture="encoder",
                        input_nc=3,
                        num_layers=3,
                        output_nc=40,
                        multiscale=True
                    )
        self.log_softmax = torch.nn.LogSoftmax(dim=-1)
    
    @property
    def conv_type(self):
        """ This is needed by the dataset to infer which batch collate should be used"""
        return self.encoder.conv_type
    
    def get_output(self):
        """ This is needed by the tracker to get access to the ouputs of the network"""
        return self.output
    
    def get_labels(self):
        """ Needed by the tracker in order to access ground truth labels"""
        return self.labels
    
    def get_current_losses(self):
        """ Entry point for the tracker to grab the loss """
        return {"loss_class": float(self.loss_class)}
    
    def forward(self, data):
        # Set labels for the tracker
        self.labels = data.y.squeeze()

        # Forward through the network
        data_out = self.encoder(data)
        self.output = self.log_softmax(data_out.x.squeeze())

        # Set loss for the backward pass
        self.loss_class = torch.nn.functional.nll_loss(self.output, self.labels)
    
    def backward(self):
         self.loss_class.backward()

model = PointNet2CLassifier()

model

NUM_WORKERS = 2
BATCH_SIZE = 8


from omegaconf import OmegaConf
params = OmegaConf.create(yaml_config)
os.mkdir("data")

yaml_config_no_aug = """
task: classification
class: modelnet.ModelNetDataset
name: modelnet
dataroot: %s
number: %s
train_transforms:
    - transform: FixedPoints
      lparams: [2048]
    - transform: AddFeatsByKeys
      params:
        feat_names: [norm]
        list_add_to_x: [%r]
        delete_feats: [True]
test_transforms:
    - transform: FixedPoints
      lparams: [2048]
    - transform: AddFeatsByKeys
      params:
        feat_names: [norm]
        list_add_to_x: [%r]
        delete_feats: [True]
""" % (os.path.join(DIR, "data"),MODELNET_VERSION, USE_NORMAL, USE_NORMAL)

from omegaconf import OmegaConf
params_no_aug = OmegaConf.create(yaml_config_no_aug)

from torch_points3d.datasets.classification.modelnet import ModelNetDataset
dataset = ModelNetDataset(params_no_aug)
dataset

dataset.create_dataloaders(
    model, 
    batch_size=BATCH_SIZE, 
    shuffle=True, 
    num_workers=NUM_WORKERS, 
    precompute_multi_scale=False
)

next(iter(dataset.test_dataloaders[0]))

# Setup the tracker and actiavte tensorboard loging
os.mkdir("logs")
logdir = "logs" # Replace with your own path
logdir = os.path.join(logdir, "RSCNN_2048_raw_1")
os.mkdir(logdir)
os.chdir(logdir)
tracker = dataset.get_tracker(False, True)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

from torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq

def train_epoch(device):
    model.to(device)
    model.train()
    tracker.reset("train")
    train_loader = dataset.train_dataloader
    iter_data_time = time.time()
    with Ctq(train_loader) as tq_train_loader:
        for i, data in enumerate(tq_train_loader):
            t_data = time.time() - iter_data_time
            iter_start_time = time.time()
            optimizer.zero_grad()
            data.to(device)
            model.forward(data)
            model.backward()
            optimizer.step()
            if i % 10 == 0:
                tracker.track(model)

            tq_train_loader.set_postfix(
                **tracker.get_metrics(),
                data_loading=float(t_data),
                iteration=float(time.time() - iter_start_time),
            )
            iter_data_time = time.time()

def test_epoch(device):
    model.to(device)
    model.eval()
    tracker.reset("test")
    test_loader = dataset.test_dataloaders[0]
    iter_data_time = time.time()
    with Ctq(test_loader) as tq_test_loader:
        for i, data in enumerate(tq_test_loader):
            t_data = time.time() - iter_data_time
            iter_start_time = time.time()
            data.to(device)
            model.forward(data)           
            tracker.track(model)

            tq_test_loader.set_postfix(
                **tracker.get_metrics(),
                data_loading=float(t_data),
                iteration=float(time.time() - iter_start_time),
            )
            iter_data_time = time.time()

EPOCHS = 10
for i in range(EPOCHS):
  print("=========== EPOCH %i ===========" % i)
  time.sleep(0.5)
  train_epoch('cuda')
  tracker.publish(i)
  test_epoch('cuda')
  tracker.publish(i)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/IFT3150/Object_Classification/logs/RSCNN_512_raw_1

